---
layout: default
title: "Fall 2022 Reading Group"
permalink: /agisf/
---

# Fall 2022 Reading Group

**The topic:** AGI (Artificial General Intelligence) Safety Fundamentals.

**Why it matters:** As powerful AI systems take on increasingly impactful tasks, we need to take great care to prevent catastrophic unintended consequences. 

**Why else you might enjoy participating:** Chipotle and/or Cafe Yumm lunches provided.

**The time commitment:** Meet five times total this term, 1-2pm on alternating Thursdays. To prepare for each meeting, spend ~1 hour on a few short readings. 

**The logistical details:** See the Event Schedule on https://www.aigsa.club/.

**Communication:** Join the conversation on the AIGSA [Discord](https://discord.gg/wGrtzFM8sJ).

## Curriculum

Kindly condensed for us by Richard Ngo from [https://www.agisafetyfundamentals.com/ai-alignment-curriculum](https://www.agisafetyfundamentals.com/ai-alignment-curriculum)

### Artificial general intelligence (Week 2: 10/6)
1.  [Four background claims (Soares, 2015)](https://intelligence.org/2015/07/24/four-background-claims/) (15 mins) 
2.  [AGI safety from first principles (Ngo, 2020)](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view) (from section 1 to end of 2.1) (20 mins)
3.  More is different for AI (Steinhardt, 2022) (only [introduction](https://bounded-regret.ghost.io/more-is-different-for-ai/), [second post](https://bounded-regret.ghost.io/future-ml-systems-will-be-qualitatively-different/), [third post](https://bounded-regret.ghost.io/thought-experiments-provide-a-third-anchor/)) (20 mins)

### Goals and misalignment (Week 4: 10/20)
1.  [Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020)](https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity) (15 mins)
2.  [Goal misgeneralization in deep reinforcement learning (Langosco et al., 2022)](https://arxiv.org/abs/2105.14111) (ending after section 3.3) (25 mins).  
	- Those with less background in reinforcement learning can skip the parts of section 2.1 focused on formal definitions.
3.  [What misalignment looks like as capabilities scale (Ngo, 2022)](https://www.alignmentforum.org/posts/KbyRPCAsWv5GtfrbG/what-misalignment-looks-like-as-capabilities-scale#Realistic_training_processes_lead_to_the_development_of_misaligned_goals) (only the section titled Realistic training processes lead to the development of misaligned goals, including phases 1, 2 and 3) (30 mins)

### Threat models and types of solutions (Week 6: 11/3)
1.  [What failure looks like (Christiano, 2019)](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like) (10 mins)
2.  [Intelligence explosion: evidence and import (Muehlhauser and Salamon, 2012)](https://drive.google.com/file/d/1QxMuScnYvyq-XmxYeqBRHKz7cZoOosHr/view?usp=sharing) (only pages 10-15) (15 mins)
3.  [AGI safety from first principles (Ngo, 2020)](https://drive.google.com/file/d/1uK7NhdSKprQKZnRjU58X7NLA1auXlWHt/view) (only section 5: Control) (15 mins)
4.  [AI alignment landscape (Christiano, 2020)](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) (35 mins)

### Learning from humans (Week 8: 11/17)
1.  Read both of the following blog posts, plus the full paper for whichever you found most interesting (if youâ€™re undecided, default to the critiques paper):
	1.  [Deep RL from human preferences: blog post (Christiano et al., 2017)](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) (10 mins)  
	2.  [AI-written critiques help humans notice flaws: blog post (Saunders et al., 2022)](https://openai.com/blog/critiques/) (10 mins)
2.  [The easy goal inference problem is still hard (Christiano, 2015)](https://www.alignmentforum.org/s/4dHMdK5TLN6xcqtyc/p/h9DesGT3WT9u2k7Hr) (10 mins)
3.  Factored cognition (Ought, 2019) ([introduction](https://ought.org/research/factored-cognition) and [scalability section](https://ought.org/research/factored-cognition/scalability)) (20 mins)

### Decomposing tasks for outer alignment; interpretability (Week 10: 12/1)
1.  [Summarizing books with human feedback: blog post (Wu et al., 2021)](https://openai.com/blog/summarizing-books/) (5 mins)
2.  [AI safety via debate (Irving et al., 2018)](https://arxiv.org/abs/1805.00899) (ending after section 3) (35 mins)
	- Those without a background in complexity theory can skip section 2.2.
3.  [Zoom In: an introduction to circuits (Olah et al., 2020)](https://distill.pub/2020/circuits/zoom-in/) (35 mins)
